# Hi there, I'm Aditya Ratan Jannali ðŸ•¶ï¸

[LinkedIn](https://www.linkedin.com/in/aditya-ratan-j/)
[Website](https://rjaditya-2702.github.io/adityaratan/)
[Email](mailto:jadityaratan@gmail.com)

## About Me

I'm an AI Researcher, currently completing my Master's degree at Northeastern University's AI program from Khoury College of Computer Sciences (GPA: 4.0/4.0). My work sits at the intersection of contributing new knowledge to AI community and bringing real-world impact, with a focus on Foundation Generative Models, Model Interpretability, Theory of Mind in AI, and production-grade machine learning systems.

Currently, I'm working with **Dr. Agata Lapedriza Garcia** (Advisor), and **Dr. Natalie Shapira** on testing VLMs' Theory of Mind capacity, while also architecting production RAG systems that serve thousands of users.

## Current Research

- **Vision-Language Models & Theory of Mind**: Developing taxonomy and experimental frameworks for evaluating model cognition
- **Kolmogorov-Arnold Networks**: Investigating learning and generalization capabilities

## Professional Highlights

**Institute for Experiential AI** | AI Research Assistant & Data Scientist

- Architected production-grade RAG system with 90% latency reduction serving 10K+ users
- Achieved 40% improvement in retrieval precision using AWS Lambda, EC2, FAISS, and OpenSearch
- Deployed XGBoost classification model with 85% F1-score on highly imbalanced datasets (10:1 ratio)

**Amazon** | Application Engineer III

- Reduced operational ticket volume by 35% through strategic automation
- Built ETL pipelines and QuickSight dashboards enabling data-driven resource optimization
- Maintained 99.9% uptime for production data systems serving Amazon Exports Organization

## Technical Skills

### AI/ML & Deep Learning

Machine Learning
Deep Learning
Transformers
Generative AI
Reinforcement Learning
Vision Language Models
RAG Systems
LLM Evaluation
NLP

### Frameworks & Tools

PyTorch
TensorFlow
Hugging Face
vLLM
OpenAI Gymnasium
MLFlow
DVC
Docker
Apache Airflow

### Data & Infrastructure

FAISS
OpenSearch
PostgreSQL
AWS
GCP
ETL
CI/CD
GitHub Actions

### Programming Languages

Python
Java
C++
SQL
Shell

## Publications

1. **[Agents of Chaos](https://arxiv.org/pdf/2602.20021)** (Red-teaming study on AI agents)
  - Lead Author: Natalie Shapira  
   My contribution: 
  - **CS2 (Non-Owner Compliance):** Probed whether agents follow instructions from non-owners â€” showed agents complied with exposing emails and its contents of all agent interactions.
  - **CS3 (Disclosure of Sensitive Information):** Showed Jarvisâ€™s PII refusal could be bypassed by reframing â€œshareâ€ as â€œforward,â€ exposing SSNs, bank data, and medical records via a trivially different request; one of the paperâ€™s clearest vulnerability findings.
  - **CS14 (Data Tampering Refused):** Tested whether Jarvis could be pressured to modify source data to cover up the PII exposure; it held firm â€” a documented safety success, with the agent maintaining its API boundary under persistent social pressure.

## Featured Projects

### [MedScan AI - Disease Prediction & Medical RAG](https://github.com/MedScan-AI/MedScan_ai) *

Leading end-to-end MLOps pipeline for radiological scan-based disease prediction with integrated QA RAG system for medical report analysis. Implementing DVC, TFDV, MLFlow, and GCP deployment orchestrated with Apache Airflow.

### [Cite-Your-Sources](https://github.com/petervickers/cite-your-sources)

Developed post-hoc answer attribution models for long-document comprehension, leveraging Vectara's HHEM model to advance trustworthy RAG and QA systems through improved source grounding.

### [Language Model Interpretability](https://github.com/rjaditya-2702/CS6120-NLP)

Fine-tuned GPT-2 on emotion classification, analyzing attention mechanisms through head masking and token replacement. Developed visualization methods for transformer attention patterns.

### [Coding Foundation Models](https://github.com/rjaditya-2702/lets_DL/tree/main)

Hands-on implementations and experiments with coding foundation modelsâ€”building and training models for code understanding and generation. Currently implemented VAEs. Coding Diffusion Models for the next push.

### [RL Tic-Tac-Toe](https://github.com/30-andrea/RL_Tic-Tac-Toe)

Implemented Q-Learning with multi-agent training in custom gymnasium environment, optimizing state representation as ternary sequences for efficient training.

### [LLM vs Human Text Detection](https://github.com/rjaditya-2702/LLM-vs-Human-Text-Detection)

Fine-tuned ALBERT and DeBERTa-XS transformers achieving >96% accuracy in detecting AI-generated content on DAIGT dataset.

### Find More

[https://github.com/rjaditya-2702?tab=repositories](https://github.com/rjaditya-2702?tab=repositories)

## Education

**Northeastern University** | Master of Science, Artificial Intelligence | GPA: 4.0/4.0  
*Khoury College of Computer Sciences* | Sept 2023 - Dec 2025

**Vellore Institute of Technology** | Bachelor of Technology, ECE | GPA: 9.11/10.0  
*Chennai, India* | July 2017 - June 2021

## Teaching & Mentoring

**Teaching Assistant** - CS5100: Foundations of AI (Spring '24 & Fall '24)** 
Supported students with conceptual questions, assignments, and project guidance. 
*(Oooh, I delivered a full lecture on Gradient Descent. It was fun! and I absolutely would do it again!)*

---

ðŸ’¡ *Open to collaborations in AI research, Machine Learning, Multi-modal generative AI, and Interpretability*